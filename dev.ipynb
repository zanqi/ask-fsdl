{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### chunk_into"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pick a start index and use n_chunks as stride to get a chunk, where n_chunks is the number of chunks you want to get."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[1, 4, 7, 10], [2, 5, 8], [3, 6, 9]]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def chunk_into(list, n_chunks):\n",
        "    \"\"\"Splits list into n_chunks pieces, non-contiguously.\"\"\"\n",
        "    for ii in range(0, n_chunks):\n",
        "        yield list[ii::n_chunks]\n",
        "\n",
        "\n",
        "list(chunk_into([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FAISS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The minimum steps to build a corpus and ask a question about it are implemented below:\n",
        "1. Build a FAISS index\n",
        "2. Take a question and encode it\n",
        "3. Search the index for the most similar document vectors to the question vector\n",
        "4. Build a prompt with the most similar documents\n",
        "5. Ask ChatGPT and return its answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(Document(page_content='The Prompt Artists\\nThis paper examines the art practices, artwork, and motivations of prolific users of the latest generation of text-to-image\\nmodels. Through interviews, observations, and a user survey, we present a sampling of the artistic styles and describe the\\ndeveloped community of practice around generative AI. We find that: 1) the text prompt andthe resulting image can be\\nconsidered collectively as an art piece ( prompts as art ), and 2) prompt templates (prompts with \u201cslots\u201d for others to fill in with\\ntheir own words) are developed to create generative art styles . We discover that the value placed by this community on unique\\noutputs leads to artists seeking specialized vocabulary to produce distinctive art pieces (e.g., by reading architectural blogs to\\nfind phrases to describe images). We also find that some artists use \u201cglitches\u201d in the model that can be turned into artistic\\nstyles of their own right. From these findings, we outline specific implications for design regarding future prompting and\\nimage editing options', metadata={'title': 'b', 'source': 'https://arxiv.org/abs/2305.07896'}),)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from langchain import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"put your key here\"\n",
        "\n",
        "embedding_engine = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-ada-002\",\n",
        "    allowed_special=\"all\",\n",
        ")\n",
        "texts = [\n",
        "    \"\"\"ON THE HIDDEN MYSTERY OF OCR IN\n",
        "3Microsoft Research, Redmond\n",
        "Large models have recently played a dominant role in natural language processing and multimodal\n",
        "vision-language learning. It remains less explored about their efficacy in text-related visual tasks. We\n",
        "conducted a comprehensive study of existing publicly available multimodal models, evaluating their\n",
        "performance in text recognition (scene text, artistic text, handwritten text), text-based visual question\n",
        "answering (document text, scene text, and bilingual text), key information extraction (receipts,\n",
        "documents, and nutrition facts) and handwritten mathematical expression recognition. Our findings\n",
        "reveal strengths and weaknesses in these models, which primarily rely on semantic understanding for\n",
        "word recognition and exhibit inferior perception of combinations of characters without semantic. They\n",
        "also display indifference towards text length and have limited capabilities in detecting fine-grained\n",
        "features in images. Consequently, these results demonstrate that even the current most powerful\n",
        "large multimodal models cannot match domain-specific methods in traditional text tasks and face\n",
        "greater challenges in more complex tasks. Most importantly, the baseline results showcased in this\n",
        "study could provide a foundational framework for the conception and assessment of innovative\n",
        "strategies targeted at enhancing zero-shot multimodal techniques. Evaluation pipeline is available at\n",
        "https://github.com/Yuliang-Liu/MultimodalOCR .\"\"\",\n",
        "    \"\"\"The Prompt Artists\n",
        "This paper examines the art practices, artwork, and motivations of prolific users of the latest generation of text-to-image\n",
        "models. Through interviews, observations, and a user survey, we present a sampling of the artistic styles and describe the\n",
        "developed community of practice around generative AI. We find that: 1) the text prompt andthe resulting image can be\n",
        "considered collectively as an art piece ( prompts as art ), and 2) prompt templates (prompts with \u201cslots\u201d for others to fill in with\n",
        "their own words) are developed to create generative art styles . We discover that the value placed by this community on unique\n",
        "outputs leads to artists seeking specialized vocabulary to produce distinctive art pieces (e.g., by reading architectural blogs to\n",
        "find phrases to describe images). We also find that some artists use \u201cglitches\u201d in the model that can be turned into artistic\n",
        "styles of their own right. From these findings, we outline specific implications for design regarding future prompting and\n",
        "image editing options\"\"\",\n",
        "]\n",
        "\n",
        "metadatas = [\n",
        "    {\"title\": \"a\", \"source\": \"https://arxiv.org/abs/2305.07895\"}, # source is required by QA langchain because it is used in the prompt\n",
        "    {\"title\": \"b\", \"source\": \"https://arxiv.org/abs/2305.07896\"},\n",
        "]\n",
        "\n",
        "index = FAISS.from_texts(texts=texts, embedding=embedding_engine, metadatas=metadatas)\n",
        "print(index.index.ntotal)\n",
        "query = \"What is art prompt?\"\n",
        "sources_and_scores = index.similarity_search_with_score(query, k=1)\n",
        "sources, _ = zip(*sources_and_scores)\n",
        "sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Art prompt refers to the practice of using text-to-image models to generate artwork based on a given text prompt. The resulting image and the text prompt are considered collectively as an art piece. Artists in this community develop prompt templates with \"slots\" for others to fill in with their own words, creating generative art styles. Artists also seek specialized vocabulary to produce distinctive art pieces and may incorporate \"glitches\" in the model as artistic styles. This practice has implications for future prompting and image editing options in design.\\nSOURCE: https://arxiv.org/abs/2305.07896'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
        "import prompts\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=256)\n",
        "chain = load_qa_with_sources_chain(\n",
        "        llm,\n",
        "        chain_type=\"stuff\",\n",
        "        verbose=False,\n",
        "        prompt=prompts.main,\n",
        "        document_variable_name=\"sources\",\n",
        "    )\n",
        "\n",
        "result = chain(\n",
        "        {\"input_documents\": sources, \"question\": query}, return_only_outputs=True\n",
        "    )\n",
        "answer = result[\"output_text\"]\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7861\n",
            "Running on public URL: https://e5897120767648e855.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://e5897120767648e855.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def greet(name):\n",
        "    return \"Hello \" + name + \"!\"\n",
        "\n",
        "demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
        "    \n",
        "demo.launch(share=True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7864\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = gr.TextArea(\n",
        "    label=\"Question\",\n",
        "    value=\"What is zero-shot chain-of-thought prompting?\",\n",
        "    show_label=True,\n",
        ")\n",
        "outputs = gr.TextArea(\n",
        "    label=\"Answer\", value=\"The answer will appear here.\", show_label=True\n",
        ")\n",
        "\n",
        "def qanda(query: str) -> str:\n",
        "    return \"hello\"\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=qanda,\n",
        "    inputs=inputs,\n",
        "    outputs=outputs,\n",
        "    title=\"Ask Questions About The Full Stack.\",\n",
        "    description=\"Get answers with sources from an LLM.\",\n",
        "    examples=[\n",
        "        \"What is zero-shot chain-of-thought prompting?\",\n",
        "        \"Would you rather fight 100 LLaMA-sized GPT-4s or 1 GPT-4-sized LLaMA?\",\n",
        "        \"What are the differences in capabilities between GPT-3 davinci and GPT-3.5 code-davinci-002?\",  # noqa: E501\n",
        "        \"What is PyTorch? How can I decide whether to choose it over TensorFlow?\",\n",
        "        \"Is it cheaper to run experiments on cheap GPUs or expensive GPUs?\",\n",
        "        \"How do I recruit an ML team?\",\n",
        "        \"What is the best way to learn about ML?\",\n",
        "    ],\n",
        "    allow_flagging=\"never\",\n",
        "    theme=gr.themes.Default(radius_size=\"none\", text_size=\"lg\"),\n",
        ")\n",
        "interface.launch()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ask-fsdl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
